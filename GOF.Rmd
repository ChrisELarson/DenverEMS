---
title: "Project"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(lubridate)
library(AER) #dispersion test
library(MASS) #NegBin
library(sandwich) #robust SE
library(arm)
```
Libraries used: tidyverse, lubridate, AER, MASS, sandwich, arm

##Data

Probably don't need to change date as I don't use it as a variable..more for merging purposes.  Should probably format the wday to Sun, Mon etc. instead of 1-7
```{r}
raw<- read.csv("C:\\Users\\Clars\\Downloads\\all_vars.csv")

#changing variables to factors
cols<- c('broncos','rockies','nuggets','precip')
  raw[,cols]<- data.frame(apply(raw[cols],2, as.factor))
   raw$wday<-as.factor(raw$wday)

calls<- raw %>% 
   mutate(Date = ymd_hms(raw$Date))
summary(calls)
```

##Overdispersion

Here is a sort of crude simulation of what a regular poisson should look like at 95% CI, The ideal ratio is 1, but nothing will be perfect.  Lambda dosen't matter for the simulation, I did use the sample size of our data. 

```{r}
Ratio = numeric()
M = 10000
for(i in 1:M){
  X = rpois(71129, lambda = 8);
  Ratio[i] = var(X)/mean(X)
}
quantile(Ratio, c(0.025, 0.975))

var(calls$n)/mean(calls$n)
```
This ratio is significantly larger than 1 and well outside the simulated 95% CI suggesting overdispersion

##Regular Poisson
  Full model with hour instead of hr_index
```{r}
#full model with unmodified hour
callsp<- glm(n ~ Year+Hour+Month+yday+wday+pick+Temp+precip+broncos+rockies+nuggets, data = calls, 
             family = 'poisson')
  summary(callsp)
```
Goodness of Fit:
```{r}
  with(callsp, cbind(res.deviance = deviance, df = df.residual, 
                     p = pchisq(deviance, df.residual, lower.tail = F)))
```
Recall the null assumes a good fit

With the indexed hr:
```{r}
Pindex<- glm(n ~ Year+hr_index+Month+yday+wday+pick+Temp+precip+broncos+rockies+nuggets, data = calls,
             family = 'poisson')
summary(Pindex)
```
GOF:
```{r, echo=FALSE}
with(Pindex, cbind(res.deviance = deviance, df = df.residual, 
                     p = pchisq(deviance, df.residual, lower.tail = F)))
```
Still not a good fit.  Using the indexed hour changes the coef values and increases the dispersion ratio


We can also test for overdispersion here:
```{r}
dispersiontest(callsp);dispersiontest(Pindex)
```
As expected, overdispersion is present

##QuasiPoisson

Gives a dispersion parameter to adjust for the Var > Mean
```{r}
callsqp<- glm(n ~ Year+Hour+Month+yday+wday+pick+Temp+precip+broncos+rockies+nuggets, data = calls,
              family = 'quasipoisson')
 summary(callsqp)
```

Comparing quasipoisson and regular poisson
```{r}
Poi = coef(callsp); QPoi = coef(callsqp)
ErrorP = se.coef(callsp); ErrorQP = se.coef(callsqp)
 
compare<- cbind(Poi, QPoi, ErrorP, ErrorQP)
 compare
```
Essentially this is what quasi poisson does: Notice the coef are the same, quasipoisson adjusts the SE to account for dispersion. 

Some Drawbacks of using the quasi-likelihood:
  it lacks a log-likelihood
  Prevents you from using likelihood-based tools; ratio tests, AIC/BIC, deviance
  
#Negative Binomial

```{r}
callsnb<- glm.nb(n ~ Year+Hour+Month+yday+wday+pick+Temp+precip+broncos+rockies+nuggets, data = calls)
  summary(callsnb)
```

Using Indexed hr:
```{r}
NBindex<- glm.nb(n ~ Year+hr_index+Month+yday+wday+pick+Temp+precip+broncos+rockies+nuggets, data = calls)
  summary(NBindex)
```

GOF:
```{r, echo=F}
with(callsnb, cbind(res.deviance = deviance, df = df.residual, 
                     p = pchisq(deviance, df.residual, lower.tail = F)));
with(NBindex, cbind(res.deviance = deviance, df = df.residual, 
                     p = pchisq(deviance, df.residual, lower.tail = F)))
```
the first is without the hr index.  Looks better than poisson, but still signifcant depature

##NegBin vs Poisson

An interesting way to explore this graphically. Create groups based on the predictors, compute the mean and variance for each group and plot the mean-variance relationship.

Groups created using "cut" with breaks at the 5/95/5 percentiles to get 20 groups approx equal in size
 (source is written down, so and so from Princeton)
 
```{r}
colSums(is.na(calls))
callsNA = na.omit(calls)  #omits 402 rows of Temperature Data

nb<-glm.nb(n ~ Year+hr_index+Month+yday+wday+pick+Temp+precip+broncos+rockies+nuggets, data = callsNA)  
 xb <- predict(nb)
  g <- cut(xb, breaks=quantile(xb,seq(0,100,5)/100))
  m <- tapply(callsNA$n, g, mean)
  v <- tapply(callsNA$n, g, var)

pr<-residuals(callsqp, 'pearson')
o <- sum(pr^2)/df.residual(callsqp)
  
plot(m, v, xlab="Mean", ylab="Variance", 
        main="Mean-Variance Relationship")

x<- seq(7, 15, .02)
z<- nb$theta
  
  lines(x, x*o, lty="dashed")
  lines(x, x*(1+x/z))
  
   legend("topleft", lty=c("dashed","solid"), 
             legend=c("Q. Poisson","Neg. Binom."), inset=0.05)
```
Neither one looks great to me but NegBin looks better.

Consider cross validation method to see which fits better
  -Use 2019 data (data not in the model)
  -Predict the 2019 data and compare predicted vs. actual
  
I haven't run any adjusted models yet:
 -removing insignificant predictors
 -considering correlations
 -Testing if categories are significant, i.e, some weekdays are, some aren't but we shouldn't remove a        weekday.  It's all or nothing IMO.  Probably ANOVA for that?
